---
title: "Taxi fare prediction - Data Analysis"
author: "Edoardo Busetti"
date: "August 7, 2019"
output:
  html_document:
    toc: true # table of content true
    toc_depth: 4  # upto three depths of headings (specified by #, ## and ###)
    number_sections: False  # if you want number sections at each table header
  pdf_document: default
---
# Introduction to the dataset
This data dictionary describes yellow taxi trip data. I downloaded the full dataframe from the New York City website, then I removed the first 6 months and randomly sampled from the 110 millions rows that where avaiable in the dataset.
This way I made an unbiased version of the second semester database for taxi rides for the year 2018.
I already cleaned the data and did feature engineering in the DataProcessing R Notebook, on this R Notebook I will analyse the data and create regression and classification models and then test them on out of the sample data.


* total_amount: The total amount charged to passengers.
* trip_distance: The elapsed trip distance in miles reported by the taximeter.
* extra : Miscellaneous extras and surcharges. Currently, this only includes the 0.50 and $1 rush hour and overnight charges.
* mta_tax: $0.50 MTA tax that is automatically triggered based on the metered rate in use
* tolls_amount: Total amount of all tolls paid in trip.
* improvement_surcharge: $0.30 improvement surcharge assessed trips at the flag drop.
* TimeOfRide_Minutes: The time spent on each ride in minutes.
* VendorID: TPEP provider that provided the record
* RatecodeID: The final rate in effect at the end of the trip.
* store_and_fwd_flag: This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,” because the vehicle did not have a connection to the server.
* PULocationID: TLC Taxi Zone in which the taximeter was engaged
* DOLocationID: TLC Taxi Zone in which the taximeter was disengaged
* payment_type: How the passenger paid for the trip.
* pickup_Month: The month when the meter was engaged.
* pickup_Time_hour: The hour when the meter was engaged.


# Setup
## Clear The Environment
```{r}
rm(list = ls())
```
## Import Libraries
```{r}
library(readr)
library(corrplot)
library(caTools)
library(ggplot2)
library(leaps)
library(Amelia)
library(texreg)
library(MASS)
library(knitr)
library(kableExtra)
library(caret)
library(dplyr)
library(ROCR) 
library(e1071) 
library(rpart) 
library(rpart.plot) 
library(randomForest) 
```
## Load the Engeneered Dataset
```{r}
Original_Taxi = readRDS(file = "Datasets/ProcessedDataset.rds")
```
## Check that there are not many missing values:
```{r}
missmap(Original_Taxi)
```
From the map we can already see that we don't have many missing values, if any at all.
We still have to check if there are some that might have slipped though, since the percentage of Missing values is rounded and could still be zero if we have very few of them.
Now we can check the actual number of missing values from each column:
```{r}
Number_Of_NAs = colSums(is.na(Original_Taxi)) 
print(Number_Of_NAs)
```
We see that we actually have 0 NAs in our dataset.

## Take out extra variables
We take out the Variables that are perfectly correlated with the total_amount or that would make the prediction trivial (Such as the tip amount, since the tips proposed on the credit card payments are a fixed percentage of the fare amount)
```{r}
PerfectlyCorrVariables = c("fare_amount","tip_amount")
Original_Taxi[,PerfectlyCorrVariables] = list(NULL)
```

## Spitting the Dataset in Train and Test set.
```{r}
# Splitting the Data
set.seed(1)
sample = sample.split(Original_Taxi$passenger_count,SplitRatio = 0.90) # splits the data in the ratio mentioned in SplitRatio. After splitting marks these rows as logical TRUE and the the remaining are marked as logical FALSE
Taxi_train = subset(Original_Taxi,sample ==TRUE)
Taxi_test  = subset(Original_Taxi, sample==FALSE)

# Reset the indexes
rownames(Taxi_train) <- NULL
rownames(Taxi_test) <- NULL

```


## Divide the features into Quantitative and Qualitative
```{r}
# The first 8 columns are numeric
Taxi_Quantitative = Taxi_train[,1:8]
Taxi_Qualitative = Taxi_train[,9:16]
```

# Analysing the relationship between independent variable and regressors
## Correlation matrix and correlation plot to check the relationships between the independent variables with the dependent one and between each others.

```{r}
Corr_Matrix = cor(Taxi_Quantitative)
corrplot(Corr_Matrix)
Coll_Total_Amount = sort(Corr_Matrix[-1,1], decreasing = T)
head(names(Coll_Total_Amount),n = 3) # Print the 3 most correlated variables with total_amount.
```

## Correlation test to check if the correlation between two variables is significant
Check if the correlation between the variables "total_amount" and "trip_distance" is statistically different from  0
```{r}
cor.test(~total_amount + trip_distance, data = Taxi_train)
```
As we can see from the results, since the p-value is really small, we reject the nuhh hypothesis and we can conclude with any reasonable level of significance that the correlation between the two variables in exam is NOT equal to 0

## Check if the relationship between total_amount and the most correlated variables is linear:
```{r}
par(mfrow=c(1,3))
plot(
  Taxi_train$total_amount ~ Taxi_train$trip_distance,
  ylab = "Total Amount Paid for a taxi trip",
  xlab = "Trip Distance",
  frame.plot = FALSE,
  pch = 16,
  col = rgb(red = 110, green = 200, blue = 110, alpha = 80, maxColorValue = 255)
)
Line1_lm = lm(Taxi_train$total_amount ~ Taxi_train$trip_distance)
abline(Line1_lm,col = "blue",lwd = 2)

plot(
  Taxi_train$total_amount ~ Taxi_train$tolls_amount,
  ylab = "Total Amount Paid for a taxi trip",
  xlab = "Total Tolls Paid",
  frame.plot = FALSE,
  pch = 16,
  col = rgb(red = 220, green = 110, blue = 110, alpha = 80, maxColorValue = 255)
)


Line2_lm = lm(Taxi_train$total_amount ~ Taxi_train$tolls_amount)
abline(Line2_lm,col = "green",lwd = 2)


plot(
  Taxi_train$total_amount ~ Taxi_train$TimeOfRide_Minutes,
  ylab = "Total Amount Paid for a taxi trip",
  xlab = "Time of Ride (Minutes)",
  frame.plot = FALSE,
  pch = 16,
  col = rgb(red = 90, green = 110, blue = 240, alpha = 80, maxColorValue = 255)
)


Line3_lm = lm(Taxi_train$total_amount ~ Taxi_train$TimeOfRide_Minutes)
abline(Line3_lm,col = "red",lwd = 2)

```
We can see that the relationship between the first two independent variables (Trip Distance and Total Tolls) and the independent variable (Total Amount Paid) is linear.
We can also see that we might be having outliers problems in our regression with the variable TimeOfRide_Minutes.

```{r}
plot(
  Taxi_train$total_amount ~ Taxi_train$TimeOfRide_Minutes,
  ylab = "Total Amount Paid for a taxi trip",
  xlab = "Time of Ride (Minutes)",
  frame.plot = FALSE,
  pch = 16,
  col = rgb(red = 90, green = 110, blue = 240, alpha = 80, maxColorValue = 255))


Model_With_Outliers = lm(Taxi_train$total_amount ~ Taxi_train$TimeOfRide_Minutes)
abline(Model_With_Outliers,col = "red",lwd = 2)
```
We can see from the plot that there are possible outliers in the data, for the variable Time Of Ride, so I decided to analyse if they actually have an impact on the regression fit or if they don't using the Leverage and Cook's distance.
```{r}
plot(Model_With_Outliers)
```
As we can see there are some points that could influence the regression, even though they are not actually outside of cook's distance they still could, once aggregated, have an effect on our regression.
Let's check how many they are and how they influence our regression.
```{r}
Outliers_Number = sum(Original_Taxi$TimeOfRide_Minutes > 180)
cat("Outliers Number: ",Outliers_Number,"\n")
OutliersData = Original_Taxi[which(Original_Taxi$TimeOfRide_Minutes > 180),]

MinutesPerMile_Outliers = mean(OutliersData$TimeOfRide_Minutes/OutliersData$trip_distance)
cat("Mean minutes per mile of outsiders: ",MinutesPerMile_Outliers)
```
We only have few outliers and since the trip distance seems to hint that the TimeOfRide variable is wrong (The mean amount of minutes per mile in the outliers dataset is around 800), I will remove those observations and rebuild the model and the plot to see if the linear fit was influenced.
```{r}


Original_Taxi = Original_Taxi[-which(Original_Taxi$TimeOfRide_Minutes > 180),]
# Redefine the subsets since we changed the data removing outliers
set.seed(1)
sample = sample.split(Original_Taxi$passenger_count,SplitRatio = 0.90)
Taxi_train = subset(Original_Taxi,sample ==TRUE)
Taxi_test  = subset(Original_Taxi, sample==FALSE)
rownames(Taxi_train) <- NULL
rownames(Taxi_test) <- NULL

Taxi_Quantitative = Taxi_train[,1:8]
Taxi_Qualitative = Taxi_train[,9:16]
```

Redoing the model and the plotting:
```{r}

Model_removed_Outliers = lm(Taxi_train$total_amount ~ Taxi_train$TimeOfRide_Minutes)

screenreg(list(Model_With_Outliers,Model_removed_Outliers),custom.model.names = c("Model With Outliers","Model without outliers"))

plot(
  Taxi_train$total_amount ~ Taxi_train$TimeOfRide_Minutes,
  ylab = "Total Amount Paid for a taxi trip",
  xlab = "Time of Ride (Minutes)",
  frame.plot = FALSE,
  pch = 16,
  col = rgb(red = 90, green = 110, blue = 240, alpha = 80, maxColorValue = 255))


abline(Model_removed_Outliers,col = "red",lwd = 2)
```
As we can see the model without outliers explains the relationship between the two variables taken into consideration way better that the model with outliers. The $R^{2}$ of the regression with outliers is 0.03, whicle the $R^{2}$ of the regression whitout outliers is 0.64.
We can also see that the coefficient of both the TimeOfRide_Minutes variable and the Intercept are drastically different.

## T-test for difference of means 
to check if the VendorID factor has any effect on the mean of total_amount.

```{r}
t.test(Taxi_train$total_amount[Taxi_train$VendorID == "Creative Mobile Technologies"],Taxi_train$total_amount[Taxi_train$VendorID == " VeriFone"], mu = 0, conf.level = 0.95, alternative = "two.sided")
```
We reject the null hypothesis that the mean price paid for a ride is the same for both levels of the VendorID factor. So we should keep this variable, since it could improve our model capability to predict the total_amount.

## T-test to check if the mean ride price in Queens is different from the mean price paid in Brooklyn 
```{r}
t.test(Taxi_train$total_amount[Taxi_train$PULocationID == "Brooklyn"],Taxi_train$total_amount[Taxi_train$PULocationID == "Queens"], mu = 0, conf.level = 0.99, alternative = "two.sided")
```
Since, the p-value for the t-test of difference of means is lower than the confidence level wanted (Confidence level = 0.99), we reject the null Hypothesis that the mean price of a ride in Brooklyn is the same as the mean price of a ride in Queens. (That is, their difference is not equal to 0).
### Plotting the different distributions for the two LocationIDs taken into account:
```{r}

Temp1 = Taxi_train[Taxi_train$PULocationID == "Brooklyn",]
Temp2 = Taxi_train[Taxi_train$PULocationID == "Queens",]
SelectedLocations = rbind(Temp1, Temp2)

Names_LocID = c("Brooklyn","Queens")
SelectedLocations = mutate(SelectedLocations, PULocationID = factor(PULocationID, level = Names_LocID ,label = Names_LocID))

# Make the density plot to show the distribution of the Taxi total fares within two areas.
ggplot(SelectedLocations, aes(total_amount, fill = PULocationID )) + geom_density(col = NA,alpha = 0.35) + xlim(0,150)
# We specify xlim so that the distribution is better zoommed in insted of including extreme values in the plot.
```

# Model Selection - Regression
## Adjusted Rsquared regsubsets
Check the best Quantitative variables to introduce into the model.
```{r}
regsubsets.out <- regsubsets( total_amount ~ .,
                              data = Taxi_Quantitative,
                              nbest = 1,
                              nvmax = NULL,
                              force.in = NULL, force.out = NULL,
                              method = 'exhaustive')
summary(regsubsets.out)
as.data.frame(summary(regsubsets.out)$outmat)
plot(regsubsets.out, scale='adjr2', main='Adjusted Rsq')
```
We can see how the model with the highest Adj Rsquared is the one that contains all the variables.
But, since by taking out 5 variables we only lose 0.01 of Adj Rsquared I would select as the quantitative variables to include in the model the ones corresponding to the model:
*total_amount ~ trip_distance + mta_tax + tolls_amount*.
leaving out the variables: 
**passenger_count**, **extra**, **mta_tax**, **improvement_surcharge**, **TimeOfRide_Minutes**

## Stepwise AIC
**Stepwise variable selection**
Now I'll use stepwise AIC to check which variables can lower the AIC score of my model
```{r}
Model_1 <- lm(total_amount ~ 1, data=Taxi_train)

Model_All <- lm(total_amount ~ ., data=Taxi_train)

scope_Var <- list(lower = Model_1, upper = Model_All)

Nsteps = dim(Taxi_train)[2]
  
# forward AIC
lm.selected_forward <- stepAIC(Model_1, 
                       direction = 'forward',
                       scope = scope_Var,
                       steps = Nsteps)

# backward AIC
lm.selected_backward <- stepAIC(Model_All, 
                       direction = 'backward',
                       scope = scope_Var,
                       steps = Nsteps)
```

## Testing the models
From this regression we found a few models to test:
1. *total_amount ~ trip_distance + RatecodeID + TimeOfRide_Minutes + PULocationID + payment_type + DOLocationID + mta_tax + tolls_amount + extra + store_and_fwd_flag + improvement_surcharge*
2. *total_amount ~ trip_distance + TimeOfRide_Minutes + RatecodeID + PULocationID + DOLocationID + payment_type + tolls_amount*
3. *total_amount ~ trip_distance + TimeOfRide_Minutes + RatecodeID + PULocationID + DOLocationID + payment_type*

The first one has the best AIC score, but it includes many variables.
The second and the third ones have lower AIC but still close enough and could generalize better on test data, since they have less variables.
```{r}
# Training Model 1
Model1 = lm(total_amount ~ trip_distance + RatecodeID + TimeOfRide_Minutes + PULocationID + payment_type + DOLocationID + mta_tax + tolls_amount + extra + store_and_fwd_flag + improvement_surcharge, data = Taxi_train) 

# Training Model 2
Model2 = lm(total_amount ~ trip_distance + TimeOfRide_Minutes + RatecodeID + PULocationID + DOLocationID + payment_type + tolls_amount + extra, data = Taxi_train)

# Training Model 3
Model3 = lm(total_amount ~ trip_distance + TimeOfRide_Minutes + RatecodeID + PULocationID + DOLocationID + payment_type, data = Taxi_train)

# Saving the Adjusted Rsquared of the models
Rsquared_Model1 = summary(Model1)$adj.r.squaredx
Rsquared_Model2 = summary(Model2)$adj.r.squared
Rsquared_Model3 = summary(Model3)$adj.r.squared

# Printing the Rsquared
cat("Adjusted R-squared: \n")
cat("Model1: ",Rsquared_Model1," Model2 : ",Rsquared_Model2," Model3: ",Rsquared_Model3)

#screenreg(list(Model1,Model2,Model3))

```
We can see that the adjusted R-quared for the models  are very similar.

### Anova test to check if the unrestricted model is better than the restricted one
We can do an anova test to check if the Model1 (Unrestricted) would better explain the variation than Model3 (Restricted):
```{r}
# Doing the anova test to check to statistically significant difference in the explainatory capacity of the constrained and unconstrained models
anova(Model1,Model3)
```
The p-values is really small so we can reject with high confidence (99.9%) the null hypothesis that the unrestricted model has no more explanatory power than the restricted model.

### Model Validation - Cross Validation 
We should now test the models doing cross validation to test how they actually perform on never seen before data:
```{r}
# Specify a function to calculate the Rsquared
rsq <- function (x, y) cor(x, y) ^ 2

#Perform k repetitions
k = 1

#Perform n fold cross
n = 5

# Defining the empty matrices on where to store the data
Rsquared_CV_Model1 = matrix(rep( 0, len=n*k), nrow = k)
RMSE_CV_Model1     = matrix(rep( 0, len=n*k), nrow = k)
Rsquared_CV_Model2 = matrix(rep( 0, len=n*k), nrow = k)
RMSE_CV_Model2     = matrix(rep( 0, len=n*k), nrow = k)
Rsquared_CV_Model3 = matrix(rep( 0, len=n*k), nrow = k)
RMSE_CV_Model3     = matrix(rep( 0, len=n*k), nrow = k)


for(j in 1:k){
#Randomly shuffle the data
Taxi_train_random <- Taxi_train[sample(nrow(Taxi_train)),]
rownames(Taxi_train_random) <- 1:nrow(Taxi_train_random) # Ordering the index names

#Create n equally size folds
folds <- cut(seq(1,nrow(Taxi_train_random)),breaks=n,labels=FALSE)

    for(i in 1:n){
    #Segement your data by fold using the which() function
      FoldIndexes <- which(folds==i)
      testData_CV  <- Taxi_train_random[FoldIndexes, ]
      trainData_CV <- Taxi_train_random[-FoldIndexes, ]
    
      # Building the models
      Model1 = lm(total_amount ~ trip_distance + RatecodeID + TimeOfRide_Minutes + PULocationID + payment_type + DOLocationID + mta_tax + tolls_amount + extra + store_and_fwd_flag + improvement_surcharge, data = trainData_CV)
      
      Model2 = lm(total_amount ~ trip_distance + TimeOfRide_Minutes + RatecodeID + PULocationID + DOLocationID + payment_type + tolls_amount + extra, data = trainData_CV)
      
      Model3 = lm(total_amount ~ trip_distance + TimeOfRide_Minutes + RatecodeID + PULocationID + DOLocationID + payment_type, data = trainData_CV)
      
      
      # Storing the results of each iteration:
      Prediction_Model1 = predict(Model1,testData_CV)
      Real_Answer = testData_CV$total_amount
      Rsquared_CV_Model1[j,i] = rsq(Prediction_Model1,Real_Answer)
      RMSE_CV_Model1[j,i] = sqrt(mean((Prediction_Model1 - Real_Answer)^2))
      
      Prediction_Model2 = predict(Model2,testData_CV)
      Rsquared_CV_Model2[j,i] = rsq(Prediction_Model2,Real_Answer)
      RMSE_CV_Model2[j,i] = sqrt(mean((Prediction_Model2 - Real_Answer)^2))
      
      Prediction_Model3 = predict(Model3,testData_CV)
      Rsquared_CV_Model3[j,i] = rsq(Prediction_Model3,Real_Answer)
      RMSE_CV_Model3[j,i] = sqrt(mean((Prediction_Model3 - Real_Answer)^2))
    }
}
# Calculating and storing the mean values of the results
Rsquared_Repetitions_CV_Model1 = rowMeans(Rsquared_CV_Model1)
Mean_Rsquared_Repetitions_CV_Model1 = mean(Rsquared_Repetitions_CV_Model1)
RMSE_Repetitions_CV_Model1 = rowMeans(RMSE_CV_Model1)
Mean_RMSE_Repetitions_CV_Model1 = mean(RMSE_Repetitions_CV_Model1)

Rsquared_Repetitions_CV_Model2 = rowMeans(Rsquared_CV_Model2)
Mean_Rsquared_Repetitions_CV_Model2 = mean(Rsquared_Repetitions_CV_Model2)
RMSE_Repetitions_CV_Model2 = rowMeans(RMSE_CV_Model2)
Mean_RMSE_Repetitions_CV_Model2 = mean(RMSE_Repetitions_CV_Model2)

Rsquared_Repetitions_CV_Model3 = rowMeans(Rsquared_CV_Model3)
Mean_Rsquared_Repetitions_CV_Model3 = mean(Rsquared_Repetitions_CV_Model3)
RMSE_Repetitions_CV_Model3 = rowMeans(RMSE_CV_Model3)
Mean_RMSE_Repetitions_CV_Model3 = mean(RMSE_Repetitions_CV_Model3)

# Creating a dataframe with the results of all the models
Model1_Analysis = c(Mean_Rsquared_Repetitions_CV_Model1,Mean_RMSE_Repetitions_CV_Model1)
Model2_Analysis = c(Mean_Rsquared_Repetitions_CV_Model2,Mean_RMSE_Repetitions_CV_Model2)
Model3_Analysis = c(Mean_Rsquared_Repetitions_CV_Model3,Mean_RMSE_Repetitions_CV_Model3)
Models_Analysis_Table = data.frame(rbind(Model1_Analysis,Model2_Analysis,Model3_Analysis))
rownames(Models_Analysis_Table) = c("Model1","Model2","Model3")
colnames(Models_Analysis_Table) = c("Rsquared Cross Validation","RMSE Cross Validation")

# Printing the results as a table
Models_Analysis_Table %>%
  kable() %>%
  kable_styling()
```
### Explaining the results
The model that performed best in the Cross Validation is Model1, now we will train the models on the whole training set and then test it on the test set to check the real world performance.
```{r}
# Training the models on the whole train dataframe
Model1_Test = lm(total_amount ~ trip_distance + RatecodeID + TimeOfRide_Minutes + PULocationID + payment_type + DOLocationID + mta_tax + tolls_amount + extra + store_and_fwd_flag + improvement_surcharge, data = Taxi_train)
nvar_model1 = 11
Model2_Test = lm(total_amount ~ trip_distance + TimeOfRide_Minutes + RatecodeID + PULocationID + DOLocationID + payment_type + tolls_amount + extra, data = Taxi_train)
nvar_model2 = 8
Model3_Test = lm(total_amount ~ trip_distance + TimeOfRide_Minutes + RatecodeID + PULocationID + DOLocationID + payment_type, data = Taxi_train)
nvar_model3 = 6

```

Making a table for model evaluation
```{r}
Real_Answer_Test = Taxi_test$total_amount
Prediction_Model1_Test = predict(Model1_Test,Taxi_test)
Rsquared_Model1_Test = rsq(Prediction_Model1_Test,Real_Answer_Test)
RMSE_Model1_Test = sqrt(mean((Prediction_Model1_Test - Real_Answer_Test)^2))

Prediction_Model2_Test = predict(Model2_Test,Taxi_test)
Rsquared_Model2_Test = rsq(Prediction_Model2_Test,Real_Answer_Test)
RMSE_Model2_Test = sqrt(mean((Prediction_Model2_Test - Real_Answer_Test)^2))

Prediction_Model3_Test = predict(Model3_Test,Taxi_test)
Rsquared_Model3_Test = rsq(Prediction_Model3_Test,Real_Answer_Test)
RMSE_Model3_Test = sqrt(mean((Prediction_Model3_Test - Real_Answer_Test)^2))

# Computing the Adjusted R_Squared for the models
num_sampl = dim(Taxi_test)[1]

Adjusted_Rsquared_Model1_Test = 1 - (1 - Rsquared_Model1_Test) * (  (num_sampl - 1)/(num_sampl - (nvar_model1  + 1)   )  )
Adjusted_Rsquared_Model2_Test = 1 - (1 - Rsquared_Model2_Test) * (  (num_sampl - 1)/(num_sampl - (nvar_model2  + 1)   )  )
Adjusted_Rsquared_Model3_Test = 1 - (1 - Rsquared_Model3_Test) * (  (num_sampl - 1)/(num_sampl - (nvar_model3  + 1)   )  )


# Making a dataframe with all the results from testing tthe models on the test set
Model1_Analysis = c(Mean_Rsquared_Repetitions_CV_Model1,Mean_RMSE_Repetitions_CV_Model1,Rsquared_Model1_Test,RMSE_Model1_Test,Adjusted_Rsquared_Model1_Test)
Model2_Analysis = c(Mean_Rsquared_Repetitions_CV_Model2,Mean_RMSE_Repetitions_CV_Model2,Rsquared_Model2_Test,RMSE_Model2_Test,Adjusted_Rsquared_Model2_Test)
Model3_Analysis = c(Mean_Rsquared_Repetitions_CV_Model3,Mean_RMSE_Repetitions_CV_Model3,Rsquared_Model3_Test,RMSE_Model3_Test,Adjusted_Rsquared_Model3_Test)
Models_Analysis_Table = data.frame(rbind(Model1_Analysis,Model2_Analysis,Model3_Analysis))
rownames(Models_Analysis_Table) = c("Model1","Model2","Model3")
colnames(Models_Analysis_Table) = c("Rsquared Cross Validation","RMSE Cross Validation","Rsquared Test set","RMSE Test set","Adjusted Rsquared Test")

# Printing a table with the results
Models_Analysis_Table %>%
  kable() %>%
  kable_styling()
```
We can see from the above table that the results for the Test set are very similar than those got in the Cross Validation, that is what we would expect, since in both cases the model is being trained on a separate set than the one it is tested on.

The best models seem to be Model1 and Model2, with very little difference in both in sample and out of sample performance.

As far as choosing just one of the two models I will chose as the best one the Model2 since it has 3 less variables and has the same performance as the more complex model (Model1).

```{r}
# Plotting the test set residuals
Test_abs_Residuals = abs(Prediction_Model2_Test - Real_Answer_Test)

plot(Test_abs_Residuals,col = "red",pch = 16,cex = 0.4,ylab = "Test Set Residuals",xlab = '')
```

As we can see from the residuals plot, most predicted values are very close to the real values (The abs(Residuals) are close to 0) and just a few observations have a high value for the residuals.

____
# Classification
Now we want to solve some classificatioon problem.
The problem I want to solve is to predict if a total_amount value is going to be higher or lower than the median.
```{r}
median_total_amount = median(Original_Taxi$total_amount)
# Density plots for the variable total_amount and its median

ggplot(Original_Taxi, aes(total_amount)) +                       
  geom_density(fill="blue", alpha=0.5) +
  geom_vline(aes(xintercept=median(total_amount, na.rm=T)), colour='darkred', linetype='dashed', size=1) +
  ggtitle("Total Amount distribution with Median line") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))

# Creating the binary variable for lower or higher than the median of total_amount
Original_Taxi$HigherThanMedian = as.factor(Original_Taxi$total_amount > median_total_amount)

# This line is just so that it looks better on the tree plot
Original_Taxi = mutate(Original_Taxi,HigherThanMedian = factor(HigherThanMedian, levels = c("TRUE","FALSE"),labels = c("T","F")))

# Dropping the total_amount variable
Original_Taxi$total_amount = NULL

# Splitting the Data now that I added another column to the whole dataset:
set.seed(1)
sample = sample.split(Original_Taxi$passenger_count,SplitRatio = 0.90)
Taxi_train = subset(Original_Taxi,sample ==TRUE)
Taxi_test  = subset(Original_Taxi, sample==FALSE)
# Reset the indexes
rownames(Taxi_train) <- NULL
rownames(Taxi_test) <- NULL

```
```{r}
# Violin plot for the HigherThanMedian Rate based on Drop Off Location and Time of Ride
ggplot(Taxi_train, aes(DOLocationID, TimeOfRide_Minutes)) + 
  geom_violin(aes(fill=HigherThanMedian), alpha=0.9) +
  facet_wrap(~HigherThanMedian) + 
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  ggtitle("HigherThanMedian Rate based on Drop Off Location and Time Of Ride") +
  xlab("Drop Off Location") +
  ylab("Time of Ride (Minutes") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```
We can see that the time of ride takes higest values for rides whose cost is higher than the median, which is what we would expect.
We can also see that most observations for taxi rides in Manhattan who had a higher cost than the median have less duration in minutes respect the other areas taken into consideration. (This can be seen from the fact the the distribution of Manhattan rides is more fat for lower values of the TimeOfRide variable.)

```{r}
# Box plot for the HigherThanMedian Rate based on the VendorID and trip_distance
ggplot(Taxi_train, aes(VendorID, trip_distance)) + 
  geom_boxplot(aes(fill=HigherThanMedian), alpha=0.9) +
  facet_wrap(~HigherThanMedian) + 
  scale_fill_manual(values=c("#56B4E9", "#CC79A7")) +
  ggtitle("HigherThanMedian Rate based on VendorID and trip_distance") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5))
```
We can see that the Higher Than Median rate in way higher for longer trip distances, but the VendorID doesn't seem to make any significant difference in the Higher Than Median rate distribution.

## First I'll do Logistic regression, KNN and Naive Bayes:

### Logistic Regression
For the logistic regression classification we will use the same variables selected for the best model in the linear regression, since the variable we are trying to predict is just a binomial version of total_amount, so the importance relationship between our dependent variable and the regressors should remain more or less the same.

**Since many of the classification algorithms that I'm about to implement are very slow I will perform the training of all the models on a seeded subsample, so that we can train them faster and still compare the results between the different algorithms.**
```{r}
set.seed(1)
# Taking a random subsample of the Train set
Reduced_Sample = Taxi_train[sample(nrow(Taxi_train), 5000), ]
```

```{r}
# We set the number of folds as 4 and the number of repeats as 1 because otherwise the model training would be too slow, idealy we would want to have a higher number of folds (~10) and a higher number of repetitions (~ 2/3)
train.control = trainControl(method = "repeatedcv", number = 4, repeats=1) # trControl parameter in the caret::train function.

# Training a logistic regression classifier
LogisticRegression_Classifier = train(HigherThanMedian ~ .,  data = Reduced_Sample, method="glm", family="binomial",trControl=train.control)

# Testing the model on the Test set:
Prediction_LogisticRegression = predict(LogisticRegression_Classifier, newdata = Taxi_test)

# Checking the prediction accuracy by printing the confusion matrix as a table
ConfusionMatrix_LogisticRegression = data.frame(table(Taxi_test$HigherThanMedian, Prediction_LogisticRegression))
names(ConfusionMatrix_LogisticRegression)[1] = "Real Value"
ConfusionMatrix_LogisticRegression %>%
  kable() %>%
  kable_styling()

# Checking the Accuracy of the model:
Error_Test_LogisticRegression = mean(Taxi_test$HigherThanMedian != Prediction_LogisticRegression)
Accuracy_LogisticRegression = round(1-Error_Test_LogisticRegression,6)
cat('\n\n The Accuracy is: ',Accuracy_LogisticRegression,'\n')

# Variable Importance in the logistic regression
plot(varImp(LogisticRegression_Classifier),top = 10)
```
We can see that the Time of Ride and trip distance are the two most important variable for this classification.

### Naive Bayes
```{r}
# fitting the Nayve Bayes to the training set:
NaiveBayes_classifier = naiveBayes(HigherThanMedian ~ ., data = Reduced_Sample)

# Testing the model on the Test set:
Prediction_NB = predict(NaiveBayes_classifier, newdata = Taxi_test)

# Checking the prediction accuracy by printing the confusion matrix
ConfusionMatrix_NaiveBayes = data.frame(table(Taxi_test$HigherThanMedian, Prediction_NB))
names(ConfusionMatrix_NaiveBayes)[1] = "Real Value"
ConfusionMatrix_NaiveBayes %>%
  kable() %>%
  kable_styling()

# Checking the Accuracy:
Error_Test = mean(Taxi_test$HigherThanMedian != Prediction_NB) # Misclassification error
Accuracy_NaiveBayes = round(1-Error_Test,6)
cat('\n\n The Accuracy is: ',Accuracy_NaiveBayes,'\n')
```

### K-Nearest Neighbours
```{r}
set.seed(1)
# Training the KNN classifier
KNN_Classifier = train(HigherThanMedian ~., data = Reduced_Sample, method = "knn",
 trControl=train.control,
 preProcess = c("center", "scale"),
 tuneLength = 10)
```

```{r}
print(KNN_Classifier) 
plot(KNN_Classifier) # Printing the parameter tuning plot

KNN_Predictions = predict(KNN_Classifier, newdata = Taxi_test)
table(Taxi_test$HigherThanMedian, KNN_Predictions)
KNN_Error <- mean(Taxi_test$HigherThanMedian != KNN_Predictions)

# Calculating and printing accuracy
Accuracy_KNN = round(1-KNN_Error,5)
cat('KNN Accuracy: ',Accuracy_KNN,"\n")
```
Here we have done K Nearest Neighbours specifying that we want to center and scale the features, that is necessary to make sure that the euclidian distance between the datapoint and the neigbours is not influenced by the mean and standard deviation of the specific feature.
Caret's train function for knn also tries different values of k to optimise for the one that gives less CV error, which in this case is k=5.

### Support Vector Machine
```{r}
SVM_Classifier <- train(HigherThanMedian ~., data = Reduced_Sample, method = "svmLinear",
                 trControl=train.control,
                 preProcess = c("center", "scale"))

SVM_Prediction = predict(SVM_Classifier, newdata = Taxi_test)


ConfusionMatrix_SVM = data.frame(table(Taxi_test$HigherThanMedian, SVM_Prediction))
names(ConfusionMatrix_SVM)[1] = "Real Value"
ConfusionMatrix_SVM %>%
  kable() %>%
  kable_styling()
Test_SVM_Error <- mean(Taxi_test$HigherThanMedian != SVM_Prediction) # Misclassification error

# Checking the accuracy of the pruned tree model
Accuracy_SVM = round(1-Test_SVM_Error,5)
cat('The Accuracy of the model is : ',Accuracy_SVM,"\n")
```



## Neural Network
```{r}
#Neural Network
library(neuralnet)
NeuralNetwork_Classifier <- neuralnet(HigherThanMedian ~trip_distance + TimeOfRide_Minutes, data=Reduced_Sample, hidden=c(2,1), linear.output=FALSE, threshold=0.5)

NeuralNetwork_Classifier$result.matrix


Prediction_NN  = as.numeric(predict(NeuralNetwork_Classifier,Taxi_test)[,2] > 0.5)
RealResults_NN = as.numeric(Taxi_test$HigherThanMedian == "T")



Test_NN_Error <- mean(RealResults_NN != Prediction_NN) # Misclassification error

# Checking the accuracy of the pruned tree model
Accuracy_NN = round(1-Test_NN_Error,5)
cat('The Accuracy of the model is : ',Accuracy_NN,"\n")

plot(NeuralNetwork_Classifier)

```
Here I did a neural network classifier trained using only the 2 most important variables (trip_distance + TimeOfRide_Minutes) and tested it on the test set, the accuracy of the model is still very high (93%) expecially if we take into consideration that we are only using 2 variables as predictors.
The Neural Network that I decided to build has only 1 Hidden layer because I think the realtionship between the variables is not that significant and can be apprended by just using one layer for the interactions.
We won't introduce the accuracy of the Neural Network into the analysis of the best classification model since we trained it on only 2 regressors while we trined the other models on all the features.



## Decision trees
### Decision trees - Pruned Tree
```{r}
# Fitting Decision Tree Classification Model to the Training set
Classifier_Tree_Model1 = train(HigherThanMedian ~., data = Reduced_Sample, method = "rpart",
                   trControl=train.control,
                   tuneLength = 10)
# Tree Visualization
prp(Classifier_Tree_Model1$finalModel, box.palette = "Reds", tweak = 1.2)
```

Using the caret function "train" with the method "rpart" I have built an automatically pruned tree (That means that it automatically chops off the insignificant branches to reduce the depth and avoid overfitting.),
now I will test the out of sample accuracy:
```{r}
PrunedTree_Prediction = predict(Classifier_Tree_Model1, newdata = Taxi_test)

# Checking the prediction accuracy and printing the confusion matrix
ConfusionMatrix_PrunedTree = data.frame(table(Taxi_test$HigherThanMedian, PrunedTree_Prediction))
names(ConfusionMatrix_PrunedTree)[1] = "Real Value"
ConfusionMatrix_PrunedTree %>%
  kable() %>%
  kable_styling()
Test_Tree_Error <- mean(Taxi_test$HigherThanMedian != PrunedTree_Prediction) # Misclassification error

# Checking the accuracy of the pruned tree model
Accuracy_PrunedTree = round(1-Test_Tree_Error,5)
cat('The Accuracy of the model is : ',Accuracy_PrunedTree,"\n")
VarImportance = varImp(Classifier_Tree_Model1)
print(Classifier_Tree_Model1)
plot(varImp(Classifier_Tree_Model1),top = 10)

```
As we can see the accuracy of this model is much higher than the Naive Bayes model and the tree we came up with is still simple enough to be easy to interpret.
We can also see that the most important varibles for the prediction are TimeOfRide_Minutes and trip_distance by a big margin.


Using caret::train is really easy to use different training algorithms and cross validate them to decide the hyperparameters, no we will also do the bagged trees and the random forest
![](Boring.gif){width=50%}

### Decision trees - Bagged Tree
```{r}

BaggedTreeClassifier = train(HigherThanMedian ~., data = Reduced_Sample, method = "treebag",
                    trControl=train.control) 

BaggedTree_Predictions = predict(BaggedTreeClassifier, newdata = Taxi_test)

# Checking the prediction accuracy and printing the Confusion matrix
ConfusionMatrix_BaggedTree = table(Taxi_test$HigherThanMedian, BaggedTree_Predictions)
names(ConfusionMatrix_BaggedTree)[1] = "Real Value"

ConfusionMatrix_BaggedTree %>%
  kable() %>%
  kable_styling()

# Calculating and printing the accuracy
BaggedTree_Error <- mean(Taxi_test$HigherThanMedian != BaggedTree_Predictions)
Accuracy_BaggedTree = round(1-BaggedTree_Error,4)
cat('Bagged Tree model Accuracy: ',Accuracy_BaggedTree,'\n')
```

Bagging improves the results of a basic decision trees algorithm by constructing a specified number classification trees by bootstrapping from the training data and then it combines their predictions to produce a final, more robust, prediction.

### Decision trees - Random forest
Since the random forest algorithm is computationally intensive I decided to not run the training function on the whole train dataset but just on a random subsample.

![The reality about Random Forest](Frozen.jpg)

```{r}
set.seed(1) # Setting the seed for results reproductability
# By specifying the "rf" parameter in caret::train we are training a decision tree based on the random forest algorithm

# Training the random forest classifier
RandomForest_Classifier = train(HigherThanMedian ~., data = Reduced_Sample, method = "rf",
                      trControl=train.control,tuneLength = 5)

RandomForest_Predictions = predict(RandomForest_Classifier, newdata = Taxi_test)

table(Taxi_test$HigherThanMedian, RandomForest_Predictions)
RandomForest_Error <- mean(Taxi_test$HigherThanMedian != RandomForest_Predictions)

Accuracy_RandomForest = round(1-RandomForest_Error,5)
cat('Random Forest Accuracy: ',Accuracy_RandomForest,"\n")

```
The Random Forest Algorithm takes a different random samples with replacement from the data to build each decision tree and then it also choses a random subset of features to decide how to split the nodes.
When doing the predictions using this algorithm we average the prediction of each decision tree.

# Analysis of Model Selection by cross validation
## Random Forest Cross Validation parameters tuning

```{r}
print(RandomForest_Classifier)
plot(RandomForest_Classifier)
```
As we can see from the plot the optimal amount of features selected is 55, where the total amount of features is 56 (1 for each numerical independent variable and n-1 for each factor where n is the number of factors levels)
The optimal amount of features is selected by doing CV testing, that is why we call this CV model selection.
We can also specify the amount of times we select a random number of features for the parameter tuning by specifying the tuneLength parameter.
```{r}
# using the random forest function just because is easier to plot the model analysis graph than using the classifier got from the caret::train function for random forest
RandomForest_Classifier_Alt = randomForest(HigherThanMedian ~ ., data = Reduced_Sample)
print(RandomForest_Classifier_Alt)

plot(RandomForest_Classifier_Alt)
legend("topleft", cex =0.5, legend= c("OOB error","True class error","False class error"), lty=c(1,2,3), col=c(1,2,3), horiz=T)
```

In this plot the black continuous line is the Out Of the Bag error (OOB)
The red line is the error on the class "True" and the green line is the error on the class "False"
As we can see the model is not really improving any more after the first ~200 trees and we can also see how the model has less error on the class False than on the class True. This can also be confirmed by the print(model) command.

## KNN Cross Validation parameters tuning
```{r}
plot(KNN_Classifier) # Printing the parameter tuning plot
```
As we can see from this plot the amount of Nearest Neighbours that gives a lower cross validation error from the ones randomly selected is 5, so the caret::train function automaticly selected that number of neighbours for our model training.

# Comparison of the out of sample accuracy of the the different classification models
```{r}
library(data.table)

# Building a dataframe with the Test set accuracy of all the models done for classification
ModelsAnalysis = data.frame(Accuracy_NaiveBayes,Accuracy_LogisticRegression,Accuracy_SVM,Accuracy_KNN,Accuracy_PrunedTree,Accuracy_BaggedTree,Accuracy_RandomForest)
#ModelsAnalysis = data.frame(transpose(ModelsAnalysis))

#colnames(ModelsAnalysis) = c("Accuracy: ")
#rownames(ModelsAnalysis) = c("Naive Bayes","Logistic Regression","K Nearest Neighbours","Pruned Tree","Bagged Tree","Random Forest")

rownames(ModelsAnalysis) = NULL

# Printing the dataframe as a table
ModelsAnalysis %>%
  kable() %>%
  kable_styling()
```
As we can see the best models seems to be the SVM and Random Forest.
SVM is faster to train but it's harder to interpret for non univariate problems, since we are niot able to plot and interpret a decision boundary.

On the other hand the random forest method has 2 problems, is less interpretable, since it would be very hard to look at all the random trees (In our model they are 500), and derive which features are the most important ones.
Another problem with the random forest algorithm is that it has a high computational cost.

We can try and derive the features importance by using the Gini impurity index:
```{r}
# Feature Importance
Gini_df = as.data.frame(importance(RandomForest_Classifier_Alt))
Gini_df = data.frame(Feature = row.names(Gini_df), 
                  MeanGini = round(Gini_df[ ,'MeanDecreaseGini'], 2))
Gini_df = Gini_df[order(-Gini_df[,"MeanGini"]),]

ggplot(Gini_df,aes(reorder(Feature,MeanGini), MeanGini, group=1)) + 
  geom_point(color='blue',shape=17,size=2) + 
  geom_line(color='red',size=1) +
  scale_y_continuous(breaks=seq(0,60,10)) +
  xlab("Feature") + 
  ggtitle("Mean Gini Index of Features") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
We can see from this plot how the most important varibles to consider are, Time Of Ride and trip distance, they stand out from all the other variables.
This is the same conslusion we got from analysing the pruned tree.

# Plotting decision boundaries

## SVM decision boundary plot
Here below you can see a plot showing how the decisionn boundary of the SVM algorithm works in the case of just 2 independent variables.
```{r}
# We reduce the sample even further so that the plot is clearer

MiniSample = Reduced_Sample[1:100,]
#Normalize features:
MiniSample[,1] = (MiniSample[,1] - mean(MiniSample[,1]))/sd(MiniSample[,1])
MiniSample[,2] = (MiniSample[,2] - mean(MiniSample[,2]))/sd(MiniSample[,2])

MiniSample$HigherThanMedian = as.factor(ifelse(MiniSample$HigherThanMedian == "T",1,0))


SVM_Classifier_TwoDim <- train(HigherThanMedian ~ trip_distance + TimeOfRide_Minutes, data = MiniSample, method = "svmLinear",
                 trControl=train.control)


```


```{r}
# installing library ElemStatLearn 
library(ElemStatLearn) 
# Plotting the training data set results 
set = select(MiniSample,trip_distance,TimeOfRide_Minutes,HigherThanMedian)
rownames(set) = NULL

X1 = seq(min(set[, 1]), max(set[, 1]) + 1, by = 0.01) 
X2 = seq(min(set[, 2]), max(set[, 2]) + 1, by = 0.01) 
  
grid_set = expand.grid(X1, X2) 
colnames(grid_set) = c('trip_distance', 'TimeOfRide_Minutes') 
y_grid = predict(SVM_Classifier_TwoDim, newdata = grid_set) 
  
plot(set[, -3],
     main = 'SVM (Training set)',
     xlim = range(X1), ylim = range(X2)) 
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE) 
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3')) 
```

In the case of using just 2 variables for the prediction we can actually plot the decision boundary for the Support Vector Machine algorithm and as we can see the classification is still pretty good even though we are just considering 2 variables, since they are very significative and also linearly separable.


## KNN decision boundary plot
```{r}
# Training the KNN classifier
KNN_Classifier_TwoDim = train(HigherThanMedian ~ trip_distance + TimeOfRide_Minutes,
 data = MiniSample,
 method = "knn",
 trControl=train.control,
 tuneLength = 10)

print(KNN_Classifier_TwoDim)
```
```{r}
# Plotting the training data set results 
set = select(MiniSample,trip_distance,TimeOfRide_Minutes,HigherThanMedian)
rownames(set) = NULL

X1 = seq(min(set[, 1]), max(set[, 1]) + 1, by = 0.01) 
X2 = seq(min(set[, 2]), max(set[, 2]) + 1, by = 0.01) 
  
grid_set = expand.grid(X1, X2) 
colnames(grid_set) = c('trip_distance', 'TimeOfRide_Minutes') 
y_grid = predict(KNN_Classifier_TwoDim, newdata = grid_set) 
  
plot(set[, -3],
     main = 'KNN (Training set)',
     xlim = range(X1), ylim = range(X2)) 
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE) 
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'coral1', 'aquamarine'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
```





# Potencial next steps in the analysis:
We could try and do more feature engeneering to better capture the relationships between the different variables
We could also try and find better ways to aggregate the POLocationIDs and DOLocationIDs variables, for example clustering into more and less expensive using KNN instead of aggregating them based on geographical zones of NYC.
We could also do a similar type of clustering for the pickup_Month factor varible since many months have similar price range and the mean difference between many months pairs is non significative.
We could split the passenger_count variable into a binary variable that only tells us if the Taxi had from 1 to 4 or from 5 to 6 passengers, since most taxis won't charge different prices from 1 to  passengers but Taxis that can transport 5 or 6 people might cost more.
We colud do a neural network model for the classification problem to avoid doing the feature selection ourselves, but that I think would be an overkill, since we already have ~95% test set accuracy just by doing a pruned decision tree which is way easier to interpret.
We could do a neural network for the regression problem and that could significatly improve our $R^{2}$ since the relationships between the independent variables and the dependent variable could not be linear and the linear regression model I did is, of course, linear.
We could also try and add non linear transofrmation of the independent variables to see if they improve the significance of the regressors, but I don't think that would be the case since I already checked the linearity of the relationship between the most important regressors and the dependent variable.




*The End!*
